[
    {
        "label": "argparse,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse.",
        "description": "argparse.",
        "detail": "argparse.",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "normalize_scores",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_cfg",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "build_image_dataset",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_cfg",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "simulate_p300_boosts",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "load_cfg",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "build_image_dataset",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "simulate_p300_boosts",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "normalize_scores",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt6.QtCore",
        "description": "PyQt6.QtCore",
        "isExtraImport": true,
        "detail": "PyQt6.QtCore",
        "documentation": {}
    },
    {
        "label": "QProcess",
        "importPath": "PyQt6.QtCore",
        "description": "PyQt6.QtCore",
        "isExtraImport": true,
        "detail": "PyQt6.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PyQt6.QtCore",
        "description": "PyQt6.QtCore",
        "isExtraImport": true,
        "detail": "PyQt6.QtCore",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFileDialog",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPlainTextEdit",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PyQt6.QtWidgets",
        "description": "PyQt6.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt6.QtWidgets",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "brainflow_scores",
        "importPath": "compute_p300",
        "description": "compute_p300",
        "isExtraImport": true,
        "detail": "compute_p300",
        "documentation": {}
    },
    {
        "label": "synth_scores",
        "importPath": "compute_p300",
        "description": "compute_p300",
        "isExtraImport": true,
        "detail": "compute_p300",
        "documentation": {}
    },
    {
        "label": "brainflow_scores_ssvep",
        "importPath": "compute_p300",
        "description": "compute_p300",
        "isExtraImport": true,
        "detail": "compute_p300",
        "documentation": {}
    },
    {
        "label": "brainflow_scores_errp",
        "importPath": "compute_p300",
        "description": "compute_p300",
        "isExtraImport": true,
        "detail": "compute_p300",
        "documentation": {}
    },
    {
        "label": "json,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json.",
        "description": "json.",
        "detail": "json.",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.calibrate",
        "description": "scripts.calibrate",
        "peekOfCode": "def main():\n    ap=argparse.ArgumentParser(); ap.add_argument('--out', default='data/processed/calibration.json'); args=ap.parse_args()\n    ensure_dir(os.path.dirname(args.out))\n    with open(args.out,'w') as f: json.dump(BARELY,f,indent=2)\n    print(f'Wrote {args.out} (barely-pass calibration)')\nif __name__=='__main__':\n    main()",
        "detail": "scripts.calibrate",
        "documentation": {}
    },
    {
        "label": "load_subset",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def load_subset(proc_dir):\n    items = list(read_jsonl(os.path.join(proc_dir, 'coco_subset.jsonl')))\n    index = {it['item_id']: it for it in items}\n    return items, index\ndef load_rsvp(proc_dir):\n    with open(os.path.join(proc_dir, 'rsvp_streams.jsonl'), 'r') as f:\n        rsvp = json.loads(next(l for l in f if l.strip()))\n    return rsvp\ndef matches_key(item, key, mode='classes'):\n    if not key:",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "load_rsvp",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def load_rsvp(proc_dir):\n    with open(os.path.join(proc_dir, 'rsvp_streams.jsonl'), 'r') as f:\n        rsvp = json.loads(next(l for l in f if l.strip()))\n    return rsvp\ndef matches_key(item, key, mode='classes'):\n    if not key:\n        return False\n    k = key.lower()\n    if mode == 'classes':\n        for c in item.get('classes', []):",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "matches_key",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def matches_key(item, key, mode='classes'):\n    if not key:\n        return False\n    k = key.lower()\n    if mode == 'classes':\n        for c in item.get('classes', []):\n            if k in str(c).lower():\n                return True\n        return False\n    else:",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "synth_scores",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def synth_scores(cfg, rsvp, items, key, match_mode):\n    # Deterministic synthetic P300 injection based on key matches\n    rng = random.Random(4242)\n    idx = {it['item_id']: it for it in items}\n    # Template amplitudes\n    amp_hit = 1.0\n    amp_miss = 0.2\n    # Accumulate simple evidence per item as sum of (post-baseline) proxies\n    evidence = {}\n    for ev in rsvp['sequence']:",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "mne_scores",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def mne_scores(args, cfg, rsvp, items):\n    raise NotImplementedError('MNE mode removed to simplify to BrainFlow/synth paths')\ndef lsl_scores(args, cfg, rsvp, items):\n    raise NotImplementedError('LSL mode removed to simplify to BrainFlow/synth paths')\ndef brainflow_scores(args, cfg, rsvp, items):\n    try:\n        from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n        from brainflow.data_filter import DataFilter, DetrendOperations, FilterTypes\n    except Exception as e:\n        raise RuntimeError('BrainFlow is required for --mode brainflow. pip install brainflow') from e",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "lsl_scores",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def lsl_scores(args, cfg, rsvp, items):\n    raise NotImplementedError('LSL mode removed to simplify to BrainFlow/synth paths')\ndef brainflow_scores(args, cfg, rsvp, items):\n    try:\n        from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n        from brainflow.data_filter import DataFilter, DetrendOperations, FilterTypes\n    except Exception as e:\n        raise RuntimeError('BrainFlow is required for --mode brainflow. pip install brainflow') from e\n    # Parameters\n    board_id_str = getattr(args, 'board_id', 'synthetic').lower()",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "brainflow_scores",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def brainflow_scores(args, cfg, rsvp, items):\n    try:\n        from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n        from brainflow.data_filter import DataFilter, DetrendOperations, FilterTypes\n    except Exception as e:\n        raise RuntimeError('BrainFlow is required for --mode brainflow. pip install brainflow') from e\n    # Parameters\n    board_id_str = getattr(args, 'board_id', 'synthetic').lower()\n    duration = float(getattr(args, 'duration', 30.0))\n    fs_override = getattr(args, 'fs', None)",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "brainflow_scores_ssvep",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def brainflow_scores_ssvep(args, cfg, rsvp, items):\n    # reuse brainflow stream capture\n    args_p300 = argparse.Namespace(board_id=getattr(args,'board_id','synthetic'), serial=getattr(args,'serial',None), duration=getattr(args,'duration',20), fs=getattr(args,'fs',None), channels=getattr(args,'channels','0,1,2,3'), key=getattr(args,'key',None), match=getattr(args,'match','classes'))\n    try:\n        from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n        from brainflow.data_filter import DataFilter, DetrendOperations, FilterTypes\n    except Exception as e:\n        raise RuntimeError('BrainFlow is required for SSVEP. pip install brainflow') from e\n    # board setup\n    bid_map={'synthetic':BoardIds.SYNTHETIC_BOARD,'cyton':BoardIds.CYTON_BOARD,'cyton_daisy':BoardIds.CYTON_DAISY_BOARD,'ganglion':BoardIds.GANGLION_BOARD}",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "brainflow_scores_errp",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def brainflow_scores_errp(args, cfg, rsvp, items):\n    # Similar stream capture as above\n    args_p = argparse.Namespace(board_id=getattr(args,'board_id','synthetic'), serial=getattr(args,'serial',None), duration=getattr(args,'duration',20), fs=getattr(args,'fs',None), channels=getattr(args,'channels','0,1,2,3'), key=getattr(args,'key',None), match=getattr(args,'match','classes'))\n    try:\n        from brainflow.board_shim import BoardShim, BrainFlowInputParams, BoardIds\n    except Exception as e:\n        raise RuntimeError('BrainFlow is required for ErrP. pip install brainflow') from e\n    bid_map={'synthetic':BoardIds.SYNTHETIC_BOARD,'cyton':BoardIds.CYTON_BOARD,'cyton_daisy':BoardIds.CYTON_DAISY_BOARD,'ganglion':BoardIds.GANGLION_BOARD}\n    bid = bid_map.get(args_p.board_id, BoardIds.SYNTHETIC_BOARD)\n    params = BrainFlowInputParams();",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.compute_p300",
        "description": "scripts.compute_p300",
        "peekOfCode": "def main():\n    ap = argparse.ArgumentParser(description='Compute item-level P300 scores')\n    ap.add_argument('--mode', choices=['synth','lsl','mne','brainflow'], default='synth')\n    ap.add_argument('--key', default='dog', help='Keyword to positively inject P300 for matching items (synth mode)')\n    ap.add_argument('--match', choices=['classes','item_id'], default='classes', help='How to match the key to items')\n    ap.add_argument('--config', default='configs/v3.yaml')\n    ap.add_argument('--out', default='data/processed/scores_p300.jsonl')\n    ap.add_argument('--proc', default='data/processed')\n    ap.add_argument('--mne-file', default=None)\n    # BrainFlow-specific",
        "detail": "scripts.compute_p300",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "scripts.control",
        "description": "scripts.control",
        "peekOfCode": "def run(cmd):\n    print('>',' '.join(cmd)); rc=subprocess.call(cmd); sys.exit(rc)\ndef main():\n    ap=argparse.ArgumentParser(description='Minimal V3 control')\n    ap.add_argument('cmd', choices=['prepare','train','calibrate','download','compute-p300','benchmark'])\n    ap.add_argument('--config', default='configs/v3.yaml')\n    ap.add_argument('--k', type=int, default=10)\n    ap.add_argument('--full', type=int, default=0)\n    # download options\n    ap.add_argument('--train', type=int, default=0, help='Download train2017 images (≈18GB)')",
        "detail": "scripts.control",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.control",
        "description": "scripts.control",
        "peekOfCode": "def main():\n    ap=argparse.ArgumentParser(description='Minimal V3 control')\n    ap.add_argument('cmd', choices=['prepare','train','calibrate','download','compute-p300','benchmark'])\n    ap.add_argument('--config', default='configs/v3.yaml')\n    ap.add_argument('--k', type=int, default=10)\n    ap.add_argument('--full', type=int, default=0)\n    # download options\n    ap.add_argument('--train', type=int, default=0, help='Download train2017 images (≈18GB)')\n    ap.add_argument('--val', type=int, default=1, help='Download val2017 images (≈1GB)')\n    ap.add_argument('--ann', type=int, default=1, help='Download annotations (≈250MB)')",
        "detail": "scripts.control",
        "documentation": {}
    },
    {
        "label": "download",
        "kind": 2,
        "importPath": "scripts.download_coco",
        "description": "scripts.download_coco",
        "peekOfCode": "def download(url: str, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(f\"✓ Exists: {dest}\")\n        return dest\n    print(f\"↓ Downloading: {url}\\n→ {dest}\")\n    urllib.request.urlretrieve(url, dest)  # no resume; simple and reliable\n    print(f\"✓ Downloaded: {dest}\")\n    return dest\ndef unzip(zpath: Path, out_dir: Path, members: tuple[str, ...] | None = None):",
        "detail": "scripts.download_coco",
        "documentation": {}
    },
    {
        "label": "unzip",
        "kind": 2,
        "importPath": "scripts.download_coco",
        "description": "scripts.download_coco",
        "peekOfCode": "def unzip(zpath: Path, out_dir: Path, members: tuple[str, ...] | None = None):\n    print(f\"⇣ Extracting: {zpath} → {out_dir}\")\n    out_dir.mkdir(parents=True, exist_ok=True)\n    with zipfile.ZipFile(zpath, 'r') as zf:\n        if members:\n            for m in members:\n                for name in zf.namelist():\n                    if name.endswith(m):\n                        zf.extract(name, out_dir)\n        else:",
        "detail": "scripts.download_coco",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.download_coco",
        "description": "scripts.download_coco",
        "peekOfCode": "def main():\n    ap = argparse.ArgumentParser(description=\"Download COCO2017 images and annotations\")\n    ap.add_argument(\"--root\", default=\"data/raw/coco2017\", help=\"Root folder for COCO2017\")\n    ap.add_argument(\"--val\", type=int, default=1, help=\"Download val2017 images (≈1GB)\")\n    ap.add_argument(\"--train\", type=int, default=0, help=\"Download train2017 images (≈18GB)\")\n    ap.add_argument(\"--ann\", type=int, default=1, help=\"Download annotations (≈250MB)\")\n    args = ap.parse_args()\n    root = Path(args.root)\n    zroot = root / \"_zips\"\n    zroot.mkdir(parents=True, exist_ok=True)",
        "detail": "scripts.download_coco",
        "documentation": {}
    },
    {
        "label": "COCO_BASE",
        "kind": 5,
        "importPath": "scripts.download_coco",
        "description": "scripts.download_coco",
        "peekOfCode": "COCO_BASE = \"http://images.cocodataset.org\"\nURLS = {\n    \"val_images\": f\"{COCO_BASE}/zips/val2017.zip\",\n    \"train_images\": f\"{COCO_BASE}/zips/train2017.zip\",\n    \"annotations\": f\"{COCO_BASE}/annotations/annotations_trainval2017.zip\",\n}\ndef download(url: str, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(f\"✓ Exists: {dest}\")",
        "detail": "scripts.download_coco",
        "documentation": {}
    },
    {
        "label": "URLS",
        "kind": 5,
        "importPath": "scripts.download_coco",
        "description": "scripts.download_coco",
        "peekOfCode": "URLS = {\n    \"val_images\": f\"{COCO_BASE}/zips/val2017.zip\",\n    \"train_images\": f\"{COCO_BASE}/zips/train2017.zip\",\n    \"annotations\": f\"{COCO_BASE}/annotations/annotations_trainval2017.zip\",\n}\ndef download(url: str, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(f\"✓ Exists: {dest}\")\n        return dest",
        "detail": "scripts.download_coco",
        "documentation": {}
    },
    {
        "label": "MinimalV3GUI",
        "kind": 6,
        "importPath": "scripts.gui",
        "description": "scripts.gui",
        "peekOfCode": "class MinimalV3GUI(QMainWindow):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"EEGCompute V3 – Minimal Control\")\n        self.resize(900, 600)\n        self.proc: QProcess | None = None\n        self._build()\n    def _build(self):\n        root = QWidget(); self.setCentralWidget(root)\n        lay = QVBoxLayout(root)",
        "detail": "scripts.gui",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.gui",
        "description": "scripts.gui",
        "peekOfCode": "def main():\n    app = QApplication(sys.argv)\n    w = MinimalV3GUI(); w.show()\n    sys.exit(app.exec())\nif __name__ == \"__main__\":\n    main()",
        "detail": "scripts.gui",
        "documentation": {}
    },
    {
        "label": "run_prepare",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def run_prepare(cfg_path: str, full: bool):\n    import subprocess\n    cmd = [\"python\",\"scripts/prepare.py\",\"--config\",cfg_path]\n    if full:\n        cmd += [\"--full\",\"1\"]\n    print('>',' '.join(cmd)); subprocess.check_call(cmd)\ndef run_calibrate():\n    import subprocess\n    print('> python scripts/calibrate.py'); subprocess.check_call([\"python\",\"scripts/calibrate.py\"])\ndef compute_eeg_scores(cfg_path: str, mode: str = 'brainflow', key: str = 'dog'):",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "run_calibrate",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def run_calibrate():\n    import subprocess\n    print('> python scripts/calibrate.py'); subprocess.check_call([\"python\",\"scripts/calibrate.py\"])\ndef compute_eeg_scores(cfg_path: str, mode: str = 'brainflow', key: str = 'dog'):\n    cfg = load_cfg(cfg_path)\n    proc = cfg['paths']['processed']\n    # Load RSVP + subset\n    import json as _json\n    with open(os.path.join(proc,'rsvp_streams.jsonl'),'r') as f:\n        rsvp = _json.loads(next(l for l in f if l.strip()))",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "compute_eeg_scores",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def compute_eeg_scores(cfg_path: str, mode: str = 'brainflow', key: str = 'dog'):\n    cfg = load_cfg(cfg_path)\n    proc = cfg['paths']['processed']\n    # Load RSVP + subset\n    import json as _json\n    with open(os.path.join(proc,'rsvp_streams.jsonl'),'r') as f:\n        rsvp = _json.loads(next(l for l in f if l.strip()))\n    items = list(read_jsonl(os.path.join(proc,'coco_subset.jsonl')))\n    val_items = [it for it in items if it.get('split')=='val']\n    # EEG streaming duration from config (faster default)",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def train_model(cfg, use_eeg_assist=False, label='baseline', log_writer=None):\n    import time\n    import importlib\n    # Import train module from the current scripts directory\n    train_mod = importlib.import_module('train')\n    # Energy estimator using psutil if available\n    try:\n        import psutil\n    except Exception:\n        psutil=None",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "predict_logits",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def predict_logits(cfg, model_dict, split='val'):\n    import torch\n    from torchvision import transforms, models\n    proc = cfg['paths']['processed']\n    tfm = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    ])\n    ds = build_image_dataset(os.path.join(proc,'coco_subset.jsonl'), transform=tfm, subset_filter=lambda r: r.get('split')==split)",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def sigmoid(x):\n    try:\n        return 1.0/(1.0+math.exp(-x))\n    except OverflowError:\n        return 0.0 if x<0 else 1.0\ndef accuracy_from_logits(logits, gts, threshold=0.5):\n    preds=[1 if sigmoid(z)>=threshold else 0 for z in logits]\n    correct=sum(1 for p,t in zip(preds,gts) if p==t)\n    return correct/max(1,len(gts))\ndef aucs_from_logits(logits, gts):",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "accuracy_from_logits",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def accuracy_from_logits(logits, gts, threshold=0.5):\n    preds=[1 if sigmoid(z)>=threshold else 0 for z in logits]\n    correct=sum(1 for p,t in zip(preds,gts) if p==t)\n    return correct/max(1,len(gts))\ndef aucs_from_logits(logits, gts):\n    # Compute AUROC and AUPRC without sklearn\n    pairs = sorted([(float(sigmoid(z)), int(t)) for z,t in zip(logits,gts)], key=lambda x: x[0], reverse=True)\n    P = sum(t for _,t in pairs); N = len(pairs)-P\n    if P==0 or N==0:\n        return 0.0, 0.0",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "aucs_from_logits",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def aucs_from_logits(logits, gts):\n    # Compute AUROC and AUPRC without sklearn\n    pairs = sorted([(float(sigmoid(z)), int(t)) for z,t in zip(logits,gts)], key=lambda x: x[0], reverse=True)\n    P = sum(t for _,t in pairs); N = len(pairs)-P\n    if P==0 or N==0:\n        return 0.0, 0.0\n    # ROC and PR points\n    tp=0; fp=0; roc=[]; prec=[]; rec=[]\n    for s,t in pairs:\n        if t==1: tp+=1",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "load_scores",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def load_scores(proc_dir, fname):\n    p = os.path.join(proc_dir, fname)\n    d={}\n    if os.path.exists(p) and os.path.getsize(p)>0:\n        for rec in read_jsonl(p): d[rec['item_id']]=float(rec.get('score',0.0))\n    return d\ndef fuse_logits_with_eeg(logits, meta, proc_dir, cfg):\n    # combine CNN logits with EEG (P300, SSVEP, ErrP) boosts\n    alpha = float(cfg.get('fusion',{}).get('alpha_p300_boost', 1.3))\n    beta  = float(cfg.get('fusion',{}).get('beta_ssvep', 0.5))",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "fuse_logits_with_eeg",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def fuse_logits_with_eeg(logits, meta, proc_dir, cfg):\n    # combine CNN logits with EEG (P300, SSVEP, ErrP) boosts\n    alpha = float(cfg.get('fusion',{}).get('alpha_p300_boost', 1.3))\n    beta  = float(cfg.get('fusion',{}).get('beta_ssvep', 0.5))\n    gamma = float(cfg.get('fusion',{}).get('gamma_errp', 0.3))\n    p300 = load_scores(proc_dir, 'scores_p300.jsonl')\n    ssvep= load_scores(proc_dir, 'scores_ssvep.jsonl')\n    errp = load_scores(proc_dir, 'scores_errp.jsonl')\n    fused=[]\n    for z, r in zip(logits, meta):",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.pipeline",
        "description": "scripts.pipeline",
        "peekOfCode": "def main():\n    ap = argparse.ArgumentParser(description='Unified pipeline: calibrate → prepare → EEG → train 4 models → eval & report')\n    ap.add_argument('--config', default='configs/v3.yaml')\n    ap.add_argument('--full', type=int, default=0)\n    ap.add_argument('--eeg-key', default='dog')\n    args = ap.parse_args()\n    cfg = load_cfg(args.config)\n    reports_dir = cfg['paths'].get('reports', 'output/reports')\n    ensure_dir(reports_dir)\n    run_calibrate()",
        "detail": "scripts.pipeline",
        "documentation": {}
    },
    {
        "label": "build_subset",
        "kind": 2,
        "importPath": "scripts.prepare",
        "description": "scripts.prepare",
        "peekOfCode": "def build_subset(cfg, split, full=False):\n    n = int(cfg['dataset']['subset_size'][split])\n    targets = set(cfg['dataset']['target_classes'])\n    ann_path = cfg['dataset']['annotations'][split]\n    # Resolve image root with auto-detection if configured path missing\n    img_root_cfg = cfg['dataset']['images'][split]\n    if not os.path.isdir(img_root_cfg):\n        # common alternative layout: data/raw/coco2017/{split}\n        alt = os.path.join(os.path.dirname(os.path.dirname(img_root_cfg)), split+'2017')\n        if os.path.isdir(alt):",
        "detail": "scripts.prepare",
        "documentation": {}
    },
    {
        "label": "build_priors",
        "kind": 2,
        "importPath": "scripts.prepare",
        "description": "scripts.prepare",
        "peekOfCode": "def build_priors(cfg, items):\n    rng = random.Random(2025)\n    priors=[]\n    for it in items:\n        base = rng.random()\n        # Stronger overlap so OFF isn't trivially perfect\n        if it['is_target']:\n            pri = 0.49 + 0.07*base  # ~0.49..0.56\n        else:\n            pri = 0.46 + 0.09*base  # ~0.46..0.55",
        "detail": "scripts.prepare",
        "documentation": {}
    },
    {
        "label": "build_rsvp",
        "kind": 2,
        "importPath": "scripts.prepare",
        "description": "scripts.prepare",
        "peekOfCode": "def build_rsvp(cfg, items):\n    rate = float(cfg['rsvp']['rate_hz']); dt=1.0/rate\n    seq=[{'t': round(i*dt,3), 'item_id': it['item_id']} for i,it in enumerate(items)]\n    return {'session':'S1','rate_hz':rate,'sequence':seq}\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument('--config', required=True)\n    ap.add_argument('--full', type=int, default=0, help='Use all available images on disk for each split')\n    args=ap.parse_args()\n    cfg=load_cfg(args.config)",
        "detail": "scripts.prepare",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.prepare",
        "description": "scripts.prepare",
        "peekOfCode": "def main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument('--config', required=True)\n    ap.add_argument('--full', type=int, default=0, help='Use all available images on disk for each split')\n    args=ap.parse_args()\n    cfg=load_cfg(args.config)\n    proc=cfg['paths']['processed']\n    ensure_dir(proc)\n    # subset\n    items = build_subset(cfg,'train', full=bool(args.full)) + build_subset(cfg,'val', full=bool(args.full))",
        "detail": "scripts.prepare",
        "documentation": {}
    },
    {
        "label": "train_cnn",
        "kind": 2,
        "importPath": "scripts.train",
        "description": "scripts.train",
        "peekOfCode": "def train_cnn(cfg, use_eeg_assist=False, log_writer=None, label='baseline'):\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader\n    from torchvision import models, transforms\n    from tqdm import tqdm\n    proc=cfg['paths']['processed']\n    # transforms (separate train/val to allow augmentation)\n    tr_cfg = cfg.get('training',{})",
        "detail": "scripts.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.train",
        "description": "scripts.train",
        "peekOfCode": "def main():\n    ap=argparse.ArgumentParser(); ap.add_argument('--config', required=True); args=ap.parse_args()\n    cfg=load_cfg(args.config)\n    models_dir=cfg['paths']['models']; ensure_dir(models_dir)\n    # logging\n    logs_dir=os.path.join(cfg['paths'].get('reports','output/reports'),'..','logs')\n    logs_dir=os.path.normpath(logs_dir)\n    ensure_dir(logs_dir)\n    log_path=os.path.join(logs_dir,'train_loss.csv')\n    # initialize CSV header if not exists",
        "detail": "scripts.train",
        "documentation": {}
    },
    {
        "label": "JsonlImageDataset",
        "kind": 6,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "class JsonlImageDataset(Dataset):\n    \"\"\"Top-level dataset class so it is picklable by DataLoader workers.\"\"\"\n    def __init__(self, path, transform=None, target_key='is_target', subset_filter=None):\n        if Image is None or Dataset is object:\n            raise RuntimeError(\"PyTorch/Pillow required. pip install torch torchvision Pillow\")\n        self.items = []\n        for r in read_jsonl(path):\n            if subset_filter and not subset_filter(r):\n                continue\n            if not os.path.exists(r['filepath']):",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def ensure_dir(p):\n    Path(p).mkdir(parents=True, exist_ok=True)\ndef write_jsonl(path, records):\n    ensure_dir(os.path.dirname(path))\n    with open(path, 'w') as f:\n        for r in records:\n            f.write(json.dumps(r)+\"\\n\")\ndef read_jsonl(path):\n    with open(path, 'r') as f:\n        for line in f:",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def write_jsonl(path, records):\n    ensure_dir(os.path.dirname(path))\n    with open(path, 'w') as f:\n        for r in records:\n            f.write(json.dumps(r)+\"\\n\")\ndef read_jsonl(path):\n    with open(path, 'r') as f:\n        for line in f:\n            line=line.strip()\n            if not line: continue",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "read_jsonl",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def read_jsonl(path):\n    with open(path, 'r') as f:\n        for line in f:\n            line=line.strip()\n            if not line: continue\n            yield json.loads(line)\ndef load_cfg(path):\n    import json as _json\n    with open(path,'r') as f:\n        return _json.load(f)",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "load_cfg",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def load_cfg(path):\n    import json as _json\n    with open(path,'r') as f:\n        return _json.load(f)\ndef now_ts():\n    return time.time()\n# ---- Image dataset helpers (PyTorch) ----\ntry:\n    from PIL import Image\n    from torch.utils.data import Dataset",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "now_ts",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def now_ts():\n    return time.time()\n# ---- Image dataset helpers (PyTorch) ----\ntry:\n    from PIL import Image\n    from torch.utils.data import Dataset\nexcept Exception:\n    Image = None\n    Dataset = object\nclass JsonlImageDataset(Dataset):",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "build_image_dataset",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def build_image_dataset(jsonl_path, transform=None, target_key='is_target', subset_filter=None):\n    return JsonlImageDataset(jsonl_path, transform, target_key, subset_filter)\n# ---- EEG Simulation Utilities ----\ndef simulate_p300_boosts(cfg, rsvp_stream, subset_items, seed: int = 1337):\n    \"\"\"\n    Produce per-item signed boosts based on RSVP events.\n    Positive for targets, negative for non-targets, with stochasticity.\n    Returns dict: item_id -> float boost in roughly [-1, 1].\n    \"\"\"\n    import random, math",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "simulate_p300_boosts",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def simulate_p300_boosts(cfg, rsvp_stream, subset_items, seed: int = 1337):\n    \"\"\"\n    Produce per-item signed boosts based on RSVP events.\n    Positive for targets, negative for non-targets, with stochasticity.\n    Returns dict: item_id -> float boost in roughly [-1, 1].\n    \"\"\"\n    import random, math\n    def sigmoid(x):\n        return 1/(1+math.exp(-x))\n    sim = cfg.get('sim', {})",
        "detail": "scripts.utils",
        "documentation": {}
    },
    {
        "label": "normalize_scores",
        "kind": 2,
        "importPath": "scripts.utils",
        "description": "scripts.utils",
        "peekOfCode": "def normalize_scores(scores, method: str = 'z_tanh'):\n    \"\"\"\n    Normalize a dict[item_id->score] to roughly [-1,1].\n    Supported: 'z_tanh' (z-score then tanh), 'minmax' (-1..1), 'standard' (z-score only).\n    \"\"\"\n    import math\n    if not scores:\n        return {}\n    vals = list(scores.values())\n    n = len(vals)",
        "detail": "scripts.utils",
        "documentation": {}
    }
]